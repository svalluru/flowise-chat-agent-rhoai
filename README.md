# Flowise Chat Agent using Granite on RHOAI

In this blog post, we’ll explore how to use **Flowise** to create a Chat Agent that interfaces with a knowledge base using **Granite** on **Red Hat OpenShift AI (RHOAI)**. Flowise provides an intuitive way to design workflows that integrate various AI models and services, and in this use case, we’ll build a conversational agent that can answer questions based on content from PDFs.

## Use Case Overview

The goal is to build a conversational agent that allows users to interact with a knowledge base, specifically one stored in PDF files. We'll use Flowise to create a **Chart** that generates a chat interface, where the agent can intelligently retrieve and answer questions from the knowledge base.

## Architecture and Workflow

This solution leverages several key components within Flowise:

1. **File Loader**: This node will load PDF files into the system, allowing the knowledge content to be ingested into the application.
2. **OpenAI Embeddings Custom**: This node will handle the creation of embeddings from the knowledge content using a language model. We will use **Nomic-embed-text-v1.5** embeddings for this task.
3. **In-Memory Vector Store**: The vector store will hold the embeddings generated from the knowledge content, enabling efficient retrieval.
4. **ChatOpenAI Custom**: This node is configured to connect to a custom **LLM (Large Language Model) Endpoint URI**, enabling the use of a specific LLM model for chat inference.
5. **Conversational Retrieval QA Chain**: This chain is used to handle the retrieval and question-answering tasks, combining the embeddings and vector store to provide relevant answers.

![image](https://github.com/user-attachments/assets/a72d485e-c699-4329-8cba-27d16280be54)

## Key Components

### **LLM Inference Server**  
The LLM inference server is hosted on **Red Hat OpenShift AI** with the **Granite Model** (Granite-3.1-8B-Instruct). Granite is a powerful model that’s designed for instructive tasks, making it well-suited for conversational agents. This model is coupled with **Nomic-embed-text-v1.5**, which is used for generating high-quality text embeddings from the knowledge content.

### **File Loader**  
The **File Loader** node is responsible for loading the PDF files into Flowise. These PDFs contain the knowledge base that will be used by the agent to respond to queries.

### **OpenAI Embeddings Custom**  
This node converts the content from the PDFs into **embeddings** using the **Nomic-embed-text-v1.5** embedding model. Embeddings are vector representations of the content, enabling more efficient search and retrieval.

### **In-Memory Vector Store**  
The **In-Memory Vector Store** stores the embeddings generated by the OpenAI Embeddings Custom node. This store serves as the core repository for all the embedded knowledge, allowing quick and relevant retrieval of information based on user queries.

### **ChatOpenAI Custom**  
The **ChatOpenAI Custom** node connects to a custom **LLM Endpoint URI** hosted on Red Hat OpenShift AI. This node sends queries to the inference server, which runs the **Granite-3.1-8B-Instruct** model. This model performs the inference to generate answers based on the information retrieved from the vector store.

### **Conversational Retrieval QA Chain**  
This node forms the backbone of the question-answering mechanism. It integrates the retrieval component and the LLM inference, enabling the conversational agent to find relevant content from the vector store and generate responses accordingly.

## Steps to Setup Environment

1. **Deploy Flowise into the OpenShift Project**:  
   First, you’ll need to deploy Flowise into your OpenShift project, here named ‘flowise’. Use the following command to apply the Flowise deployment YAML file:

   ```bash
   oc apply -f flowise.yaml
   oc expose svc/flowise-service

![image](https://github.com/user-attachments/assets/3a1dab28-1c4b-4ecd-a042-43cf51bb3b6e)

2. **Access Flowise UI**:
Once Flowise is deployed, navigate to **Routes** in your OpenShift project 'flowise'. There, you will find the URL for the Flowise UI. Open this route in a web browser to access the Flowise interface.

3. **Import the Chatflow**:
In the Flowise UI, import the **Tech Support Chat Chatflow JSON** (e.g., **Tech Support Chat**) into the Flowise environment <img width="273" alt="image" src="https://github.com/user-attachments/assets/87e3c4bb-3a7f-4909-89f9-f89d776a16ce" />.<br>
This Chatflow contains the workflow for generating the chat interface, integrating the necessary nodes for question answering.

4. **Configure Nodes in Chatflow**:
Open the **Tech Support Chat** flow inside Flowise and configure the following nodes:

    - **File Loader**: Set this up to load the PDF files containing your knowledge base.
    - **OpenAI Embeddings Custom**: Set this to use the **Nomic-embed-text-v1.5** embedding model.
    - **In-Memory Vector Store**: Configure this to store the embeddings.
    - **ChatOpenAI Custom**: Configure this node to use the **Granite-3.1-8B-Instruct** model hosted on Red Hat OpenShift AI. Set up the LLM endpoint URI.
    - **Conversational Retrieval QA Chain**: Link the nodes in this chain to perform the retrieval and QA tasks.

5. **Upsert Vector Store**:
After configuring the nodes, click the **Upsert Vector Store** button <img width="30" alt="image" src="https://github.com/user-attachments/assets/18e9736e-2ff2-4d1e-a6ed-dae04e6d728f" />
to load the knowledge base content into the **In-Memory Vector Store**. This step will convert the knowledge base into embeddings and store them in the vector store for efficient retrieval.

6. **Click Chat Icon and Start the Chat**:
Once the knowledge base has been loaded, click the **Chat** icon <img width="30" alt="image" src="https://github.com/user-attachments/assets/e3590a6a-796e-4aad-8d3e-4726fa33332d" />
 in the Flowise UI to bring up the chat interface. This will allow users to begin interacting with the chat agent and ask questions based on the knowledge base.

7. **Engage with the Chat**:
You can now interact with the chat agent, and it will respond using the relevant content from the PDFs, powered by the Granite model on Red Hat OpenShift AI.

## Conclusion
By leveraging **Flowise** in conjunction with **Granite** on **Red Hat OpenShift AI**, we’ve created an efficient and scalable conversational agent that interacts with a knowledge base stored in PDFs. This setup ensures that the agent can intelligently answer questions using embeddings and LLM inference, providing users with accurate and relevant responses based on the stored knowledge.

With Flowise's intuitive chart-based design, it becomes easy to configure and deploy such AI-driven systems, while Red Hat OpenShift AI ensures the infrastructure is robust and scalable for enterprise use cases.
